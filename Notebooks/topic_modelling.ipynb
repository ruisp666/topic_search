{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Topic Modelling.\n",
    "In this notebook, we will perform topic modelling individually on each of the 3 sections that are to be considered.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link = '../all_filings_and_sections.csv'\n",
    "df = pd.read_csv(link)\n",
    "\n",
    "df['filedAt'] = pd.to_datetime(df['filedAt'], infer_datetime_format=True)\n",
    "df = df.drop(columns='Unnamed: 0')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T00:38:59.910874Z",
     "end_time": "2023-08-31T00:39:05.471085Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BertTopic(requires sentence_transformers and Bertopic)\n",
    "We get the topics. We will use  Bertopic, but will add a stop word removal so that they do not appear in the topics. They will however appear when being encoded for the sentence transformer. For production, we would use the 'all-mpnet-base-v2' as a sentence transformer, which as of moment is the top performer in [Sentence Embeddings](https://www.sbert.net/docs/pretrained_models.html). For the purposes of this MVP, we will use the \"all-MiniLM-L6-v2\", which takes approximately 20 minutes to finish per section, vs 1h:40m on an MPS device. We then add maximal marginal relevance, which works by selecting representatives of a given cluster, that are diverse as measured by their cosine similarity. In practice, we get N sentence embeddings but that are not too similar, so that the use of diverse information is maximized.\n",
    "\n",
    "Let's start by preprocessing the content of the sections, since we cannot embed them directly, without running the risk of truncating a significant part of the document. We will use Langchain for this purpose. We can use the sentence transformer of our choice so that when we compute the embeddings we know that there is no risk of truncation.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-29T20:17:04.494938Z",
     "end_time": "2023-08-29T20:17:08.809064Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to use sentence transformers, we need to extract the page content from each document. As of now, this cannot be done in a vectorized way.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "# Load the splitter to use\n",
    "splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "sections = [f'Section{s}' for s in ['1', '1A', '7']]\n",
    "split_sections = {}\n",
    "\n",
    "# For each section load the corresponding column and split the documents using the sentence-transformer\n",
    "for s in sections:\n",
    "    loader_section_s = DataFrameLoader(df, page_content_column=s)\n",
    "    docs_section_s = loader_section_s.load()\n",
    "    split_sections[s] = splitter.split_documents(docs_section_s)\n",
    "\n",
    "split_sections_text = {s: {'text': [doc.page_content for doc in v],\n",
    "                           'meta': [doc.metadata for doc in v]} for s, v in split_sections.items()}\n",
    "with open(\"../data/split_sections_text.pickle\", \"wb\") as file:\n",
    "    pickle.dump(split_sections_text, file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T23:52:20.337852Z",
     "end_time": "2023-08-30T23:59:04.264652Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can compute the embeddings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Pre-calculate embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = {s: embedding_model.encode(v['text'], show_progress_bar=True) for s,v in split_sections_text.items()}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T11:21:56.181673Z",
     "end_time": "2023-08-30T12:29:31.451062Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We save them in case we need to use them later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"../data/embeddings.pickle\", \"wb\") as file:\n",
    "    pickle.dump(embeddings, file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T14:42:20.003847Z",
     "end_time": "2023-08-30T14:42:20.320536Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now load the required elements, namely the count vectorizer and the representation model. We include up to 2 n-grams so that we can get topics consisting of two-words.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T14:30:36.316661Z",
     "end_time": "2023-08-30T14:30:38.233329Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now fit and save our 3 topic models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for s,v in split_sections_text.items():\n",
    "    topic_model = BERTopic(embedding_model=embedding_model, representation_model=representation_model, vectorizer_model=vectorizer_model)\n",
    "    topic_model.fit(v['text'], embeddings[s])\n",
    "    topic_model.save(f'topic_models/topic_models_{s}', serialization='safetensors', save_ctfidf=True, save_embedding_model=embedding_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T14:31:29.322597Z",
     "end_time": "2023-08-30T14:36:59.041256Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's save the model before we go into the topic analysis. Next we move into [analyzing](topic_analysis.ipynb) the topics."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
